{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fajaralgii04/bigdata-uts-ganjil-2526-/blob/main/Kelompok5_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** KODE SCRAP UNTUK MEMANGGIL DATA DARI API STEAM **"
      ],
      "metadata": {
        "id": "Rdt83JLldkxn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "wBaI2dcR78MK",
        "outputId": "0ab387dc-de63-4dc7-bc70-918312519e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Fetch up to 2100 for app_id=570 ...\n",
            " -> Got 2093 rows for Dota 2\n",
            "[INFO] Fetch up to 2100 for app_id=1172470 ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-362688109.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mapp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mAPP_IDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[INFO] Fetch up to {MAX_REVIEWS_PER_APP} for app_id={app} ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mdf_app\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_reviews_for_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_REVIEWS_PER_APP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0mgot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_app\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_app\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgot\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-362688109.py\u001b[0m in \u001b[0;36mfetch_reviews_for_app\u001b[0;34m(app_id, max_reviews)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_RETRIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHEADERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_redirects\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# ðŸ”¹ STEAM REVIEW SCRAPER â€” schema identik dgn Kaggle referensi\n",
        "#    Kolom: 11 kolom [appid, review, word_count, voted_up, votes_up,\n",
        "#                     votes_funny, timestamp_created, author_playtime_forever,\n",
        "#                     name, price, release_date]\n",
        "#    Catatan:\n",
        "#    - price = cent (int64), bukan dibagi 100\n",
        "#    - author_playtime_forever = menit (int64), bukan jam\n",
        "#    - release_date = float64 (NaN), disamakan dengan file referensi\n",
        "# ============================================================\n",
        "\n",
        "# âœ… Versi ini TIDAK memakai Google Drive. Output -> data/raw/ di runtime/repo.\n",
        "#    Cocok untuk notebook utama yang \"Run All\" di mesin siapa pun.\n",
        "\n",
        "import os, time, requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, Any\n",
        "from urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "# 1) Lokasi simpan lokal (reproducible)\n",
        "BASE_DIR = \"data/raw\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "OUTPUT_PATH = os.path.join(BASE_DIR, \"steam_reviews_realtime_11cols.csv\")\n",
        "\n",
        "# 2) App list (bebas ganti/extend)\n",
        "APP_IDS = [\n",
        "    570,      # Dota 2\n",
        "    1172470,  # Apex Legends\n",
        "    578080,   # PUBG: BATTLEGROUNDS\n",
        "    271590,   # Grand Theft Auto V\n",
        "]\n",
        "\n",
        "# 3) Parameter scraping\n",
        "MAX_REVIEWS_PER_APP = 2100\n",
        "LANGUAGE = \"english\"\n",
        "SLEEP_SEC = 1.0\n",
        "MAX_RETRIES = 3\n",
        "BACKOFF_BASE = 1.5\n",
        "STORE_CC = \"us\"   # gunakan 'us' agar price_overview lebih konsisten; tetap output dalam cent\n",
        "\n",
        "# ðŸ”§ HTTP session (retry + UA)\n",
        "session = requests.Session()\n",
        "retry = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=0.6,\n",
        "    status_forcelist=(429, 500, 502, 503, 504),\n",
        "    allowed_methods=frozenset([\"GET\"]),\n",
        ")\n",
        "session.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 SteamReviewCollector/1.0 (Colab or Jupyter)\"}\n",
        "\n",
        "# 4) Skema & dtypes target (persis 11 kolom)\n",
        "REQUIRED_COLS = [\n",
        "    \"appid\", \"review\", \"word_count\", \"voted_up\", \"votes_up\", \"votes_funny\",\n",
        "    \"timestamp_created\", \"author_playtime_forever\", \"name\", \"price\", \"release_date\"\n",
        "]\n",
        "\n",
        "# 5) Ambil metadata app (name, price in cent, release_date=NaN biar float64)\n",
        "_cache_appinfo: Dict[int, Dict[str, Any]] = {}\n",
        "\n",
        "def get_app_details(app_id: int) -> Dict[str, Any]:\n",
        "    if app_id in _cache_appinfo:\n",
        "        return _cache_appinfo[app_id]\n",
        "\n",
        "    url = \"https://store.steampowered.com/api/appdetails\"\n",
        "    params = {\"appids\": app_id, \"cc\": STORE_CC, \"l\": \"en\"}\n",
        "\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            r = session.get(url, headers=HEADERS, params=params, timeout=20)\n",
        "            r.raise_for_status()\n",
        "            j = r.json()\n",
        "            node = j.get(str(app_id), {})\n",
        "            if node.get(\"success\"):\n",
        "                data = node.get(\"data\", {}) or {}\n",
        "                pov = data.get(\"price_overview\") or {}\n",
        "                is_free = bool(data.get(\"is_free\", False))\n",
        "\n",
        "                # 'final' = cent; simpan sebagai int\n",
        "                final_cent = pov.get(\"final\")\n",
        "                if is_free or final_cent is None:\n",
        "                    price_cent = 0\n",
        "                else:\n",
        "                    price_cent = int(float(final_cent))\n",
        "\n",
        "                result = {\n",
        "                    \"name\": data.get(\"name\") or \"\",\n",
        "                    \"price\": price_cent,    # int\n",
        "                    \"release_date\": np.nan  # float64 NaN (samakan referensi)\n",
        "                }\n",
        "                _cache_appinfo[app_id] = result\n",
        "                return result\n",
        "        except Exception:\n",
        "            time.sleep(BACKOFF_BASE ** attempt)\n",
        "\n",
        "    # fallback\n",
        "    result = {\"name\": \"\", \"price\": 0, \"release_date\": np.nan}\n",
        "    _cache_appinfo[app_id] = result\n",
        "    return result\n",
        "\n",
        "# 6) Scrape review per app\n",
        "def fetch_reviews_for_app(app_id: int, max_reviews: int) -> pd.DataFrame:\n",
        "    rows, cursor, pulled = [], \"*\", 0\n",
        "    seen = set()\n",
        "\n",
        "    meta = get_app_details(app_id)\n",
        "    g_name, g_price, g_release = meta[\"name\"], meta[\"price\"], meta[\"release_date\"]\n",
        "\n",
        "    while pulled < max_reviews:\n",
        "        url = f\"https://store.steampowered.com/appreviews/{app_id}\"\n",
        "        params = {\n",
        "            \"json\": 1,\n",
        "            \"cursor\": cursor,\n",
        "            \"num_per_page\": 100,\n",
        "            \"filter\": \"recent\",\n",
        "            \"language\": LANGUAGE,\n",
        "            \"review_type\": \"all\",\n",
        "            \"purchase_type\": \"all\",\n",
        "        }\n",
        "\n",
        "        ok, data = False, None\n",
        "        for attempt in range(MAX_RETRIES):\n",
        "            try:\n",
        "                resp = session.get(url, headers=HEADERS, params=params, timeout=30)\n",
        "                resp.raise_for_status()\n",
        "                data = resp.json()\n",
        "                ok = True\n",
        "                break\n",
        "            except Exception:\n",
        "                time.sleep(BACKOFF_BASE ** attempt)\n",
        "\n",
        "        if not ok or not data or \"reviews\" not in data:\n",
        "            break\n",
        "\n",
        "        revs = data.get(\"reviews\") or []\n",
        "        if not revs:\n",
        "            break\n",
        "\n",
        "        for rv in revs:\n",
        "            txt = (rv.get(\"review\") or \"\").strip()\n",
        "            if not txt:\n",
        "                continue\n",
        "\n",
        "            # playtime dalam MENIT (int)\n",
        "            pt_min = (rv.get(\"author\") or {}).get(\"playtime_forever\")\n",
        "            try:\n",
        "                pt_min = int(pt_min) if pt_min is not None else 0\n",
        "            except Exception:\n",
        "                pt_min = 0\n",
        "\n",
        "            rows.append({\n",
        "                \"appid\": int(app_id),\n",
        "                \"review\": txt,\n",
        "                \"word_count\": int(len(txt.split())),\n",
        "                \"voted_up\": bool(rv.get(\"voted_up\")),\n",
        "                \"votes_up\": int(rv.get(\"votes_up\") or 0),\n",
        "                \"votes_funny\": int(rv.get(\"votes_funny\") or 0),\n",
        "                \"timestamp_created\": int(rv.get(\"timestamp_created\") or 0),\n",
        "                \"author_playtime_forever\": pt_min,   # menit (int)\n",
        "                \"name\": g_name,\n",
        "                \"price\": int(g_price or 0),          # cent (int)\n",
        "                \"release_date\": g_release            # float64 NaN\n",
        "            })\n",
        "\n",
        "        pulled += len(revs)\n",
        "\n",
        "        next_cursor = data.get(\"cursor\")\n",
        "        if not next_cursor or next_cursor in seen:\n",
        "            break\n",
        "        seen.add(next_cursor)\n",
        "        cursor = next_cursor\n",
        "\n",
        "        time.sleep(SLEEP_SEC)\n",
        "        if pulled >= max_reviews:\n",
        "            break\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # Pastikan semua kolom ada & urutan sama\n",
        "    for c in REQUIRED_COLS:\n",
        "        if c not in df.columns:\n",
        "            df[c] = np.nan\n",
        "    df = df[REQUIRED_COLS]\n",
        "\n",
        "    return df\n",
        "\n",
        "# 7) Jalankan untuk semua app\n",
        "frames = []\n",
        "for app in APP_IDS:\n",
        "    print(f\"[INFO] Fetch up to {MAX_REVIEWS_PER_APP} for app_id={app} ...\")\n",
        "    df_app = fetch_reviews_for_app(app, MAX_REVIEWS_PER_APP)\n",
        "    got = len(df_app)\n",
        "    label = df_app[\"name\"].iloc[0] if got else str(app)\n",
        "    print(f\" -> Got {got} rows for {label}\")\n",
        "    frames.append(df_app)\n",
        "\n",
        "df_s = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=REQUIRED_COLS)\n",
        "\n",
        "# 8) Enforce dtype PERSIS (int64/bool/object/float64)\n",
        "# - int64 tidak menerima NaN â†’ isi 0 dulu supaya dtype tetap int64\n",
        "int_cols = [\"appid\",\"word_count\",\"votes_up\",\"votes_funny\",\"timestamp_created\",\"author_playtime_forever\",\"price\"]\n",
        "for c in int_cols:\n",
        "    df_s[c] = pd.to_numeric(df_s[c], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
        "\n",
        "df_s[\"voted_up\"] = df_s[\"voted_up\"].astype(bool)\n",
        "df_s[\"review\"] = df_s[\"review\"].astype(object)\n",
        "df_s[\"name\"] = df_s[\"name\"].astype(object)\n",
        "df_s[\"release_date\"] = pd.to_numeric(df_s[\"release_date\"], errors=\"coerce\").astype(\"float64\")\n",
        "\n",
        "# Optional: drop duplicates (appid + timestamp + review)\n",
        "if not df_s.empty:\n",
        "    before = len(df_s)\n",
        "    df_s = df_s.drop_duplicates(subset=[\"appid\",\"timestamp_created\",\"review\"])\n",
        "    after = len(df_s)\n",
        "    if after != before:\n",
        "        print(f\"[INFO] Dropped duplicates: {before - after}\")\n",
        "\n",
        "# 9) Simpan ke lokal (reproducible)\n",
        "df_s = df_s[REQUIRED_COLS]\n",
        "df_s.to_csv(OUTPUT_PATH, index=False)\n",
        "print(f\"\\nâœ… Saved to: {OUTPUT_PATH}\")\n",
        "print(f\"Total rows: {len(df_s)}\")\n",
        "\n",
        "print(\"\\n[SUMMARY] dtypes:\")\n",
        "print(df_s.dtypes)\n",
        "\n",
        "print(\"\\n[SUMMARY] Rows per app_id:\")\n",
        "print(df_s.groupby(\"appid\")[\"review\"].count().sort_values(ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** KODE UNTUK MENGGABUNGKAN DATA SET KAGGLE DAN SCRAP **"
      ],
      "metadata": {
        "id": "0Ihd3j26d2CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# ðŸ”¹ MERGE DATASET: Kaggle + Steam Scraping (from GitHub)\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from urllib.parse import quote\n",
        "\n",
        "# --- 0) Konfigurasi sumber GitHub (PUBLIC) ---\n",
        "BASE_URL = \"https://raw.githubusercontent.com/fajaralgii04/bigdata-uts-ganjil-2526-/main/data/semua%20csv%20(gabungan)/\"\n",
        "\n",
        "def read_csv_github(filename: str, **kwargs) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load CSV dari folder GitHub kamu.\n",
        "    Otomatis handle spasi & tanda kurung pada path.\n",
        "    \"\"\"\n",
        "    url = BASE_URL + quote(filename)\n",
        "    print(f\"ðŸ“‚ Loading: {url}\")\n",
        "    return pd.read_csv(url, **kwargs)\n",
        "\n",
        "# --- 1) Nama file di repo (ubah jika namanya berbeda) ---\n",
        "kaggle_file = \"steam_game_reviews_top10_21000.csv\"      # pastikan file ini sudah diupload\n",
        "scrape_file = \"steam_reviews_realtime_11cols.csv\"       # sudah ada di repo kamu\n",
        "\n",
        "# --- 2) Baca dataset dari GitHub ---\n",
        "try:\n",
        "    df_kaggle = read_csv_github(kaggle_file)\n",
        "except Exception as e:\n",
        "    raise FileNotFoundError(\n",
        "        f\"File Kaggle tidak ditemukan di repo: {kaggle_file}\\n\"\n",
        "        f\"â†’ Upload ke folder GitHub yang sama, atau ganti variabel 'kaggle_file'.\\nDetail: {e}\"\n",
        "    )\n",
        "\n",
        "df_scrape = read_csv_github(scrape_file)\n",
        "\n",
        "print(\"Kaggle shape :\", df_kaggle.shape)\n",
        "print(\"Scrape shape :\", df_scrape.shape)\n",
        "\n",
        "# --- 3) Samakan nama kolom & tipe data (jaga konsistensi) ---\n",
        "common_cols = [\n",
        "    \"appid\",\"review\",\"word_count\",\"voted_up\",\"votes_up\",\"votes_funny\",\n",
        "    \"timestamp_created\",\"author_playtime_forever\",\"name\",\"price\",\"release_date\"\n",
        "]\n",
        "\n",
        "# pilih kolom yang diperlukan (akan error kalau tidak ada â†’ cek dulu)\n",
        "missing_k = [c for c in common_cols if c not in df_kaggle.columns]\n",
        "missing_s = [c for c in common_cols if c not in df_scrape.columns]\n",
        "if missing_k or missing_s:\n",
        "    raise ValueError(f\"Kolom hilang.\\nKaggle missing: {missing_k}\\nScrape missing: {missing_s}\")\n",
        "\n",
        "df_kaggle = df_kaggle[common_cols].copy()\n",
        "df_scrape = df_scrape[common_cols].copy()\n",
        "\n",
        "# tipe data kunci\n",
        "df_kaggle[\"voted_up\"] = df_kaggle[\"voted_up\"].astype(bool)\n",
        "df_scrape[\"voted_up\"] = df_scrape[\"voted_up\"].astype(bool)\n",
        "\n",
        "# --- 4) Gabungkan ---\n",
        "df_all = pd.concat([df_kaggle, df_scrape], ignore_index=True)\n",
        "\n",
        "print(\"\\nâœ… Gabungan Berhasil!\")\n",
        "print(f\"Total Rows   : {len(df_all):,}\")\n",
        "print(f\"Total Columns: {len(df_all.columns)}\")\n",
        "\n",
        "# --- 5) Cek missing values per kolom ---\n",
        "print(\"\\n[INFO] Missing Value (%) per Kolom:\")\n",
        "print((df_all.isna().mean() * 100).round(2))\n",
        "\n",
        "# --- 6) Simpan ke folder lokal (runtime) agar reproducible ---\n",
        "save_dir = \"data/processed\"        # lokal di runtime/notebook, tidak butuh Drive\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "output_path = os.path.join(save_dir, \"steam_reviews_combined.csv\")\n",
        "df_all.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\nðŸ’¾ File gabungan disimpan di: {output_path}\")\n"
      ],
      "metadata": {
        "id": "evLnSP77d0vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** CLEANING **"
      ],
      "metadata": {
        "id": "CFw60_FllNUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# ðŸ”¹ CLEANING â€” tanpa Drive (lokal + fallback GitHub)\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from urllib.parse import quote\n",
        "\n",
        "# --- 0) Sumber data: lokal dulu, kalau tidak ada â†’ GitHub raw ---\n",
        "LOCAL_IN   = \"data/processed/steam_reviews_combined.csv\"\n",
        "LOCAL_OUT  = \"data/processed/steam_reviews_clean.csv\"\n",
        "\n",
        "BASE_URL = \"https://raw.githubusercontent.com/fajaralgii04/bigdata-uts-ganjil-2526-/main/data/semua%20csv%20(gabungan)/\"\n",
        "GH_FILE  = \"steam_reviews_combined.csv\"  # sesuaikan jika beda nama\n",
        "\n",
        "def read_csv_github(filename: str, **kwargs) -> pd.DataFrame:\n",
        "    url = BASE_URL + quote(filename)\n",
        "    print(f\"ðŸŒ Loading from GitHub: {url}\")\n",
        "    return pd.read_csv(url, **kwargs)\n",
        "\n",
        "# 1) Load data\n",
        "if os.path.exists(LOCAL_IN):\n",
        "    print(f\"ðŸ“‚ Loading local: {LOCAL_IN}\")\n",
        "    df = pd.read_csv(LOCAL_IN)\n",
        "else:\n",
        "    df = read_csv_github(GH_FILE)\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "\n",
        "# 2) Cek struktur & null (ringkas)\n",
        "print(\"\\n[INFO] Missing values (%):\")\n",
        "print((df.isna().mean() * 100).round(2))\n",
        "\n",
        "# 3) Hapus baris tanpa teks review\n",
        "if \"review\" not in df.columns:\n",
        "    raise KeyError(\"Kolom 'review' tidak ditemukan.\")\n",
        "df = df.dropna(subset=[\"review\"])\n",
        "\n",
        "# 4) Hapus duplikat (appid + review + timestamp)\n",
        "for col in [\"appid\", \"review\", \"timestamp_created\"]:\n",
        "    if col not in df.columns:\n",
        "        raise KeyError(f\"Kolom '{col}' wajib ada untuk dedup.\")\n",
        "df = df.drop_duplicates(subset=[\"appid\", \"review\", \"timestamp_created\"])\n",
        "\n",
        "# 5) Tipe data konsisten\n",
        "df[\"voted_up\"] = df[\"voted_up\"].astype(bool)\n",
        "\n",
        "# timestamp_created â†’ datetime (jika numeric epoch detik)\n",
        "if pd.api.types.is_numeric_dtype(df[\"timestamp_created\"]):\n",
        "    df[\"timestamp_created\"] = pd.to_datetime(df[\"timestamp_created\"], unit=\"s\", errors=\"coerce\")\n",
        "\n",
        "# numerik: word_count, price, author_playtime_forever\n",
        "for c in [\"word_count\", \"price\", \"author_playtime_forever\"]:\n",
        "    if c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# 6) Tangani outlier playtime (trim 1%â€“99%) â€” hanya jika cukup baris\n",
        "if \"author_playtime_forever\" in df.columns and len(df) >= 100:\n",
        "    q1  = df[\"author_playtime_forever\"].quantile(0.01)\n",
        "    q99 = df[\"author_playtime_forever\"].quantile(0.99)\n",
        "    df  = df[df[\"author_playtime_forever\"].between(q1, q99, inclusive=\"both\")]\n",
        "\n",
        "# 7) Isi NaN numerik yang tersisa dengan 0 (aman untuk modeling dasar)\n",
        "for c in [\"word_count\", \"price\", \"votes_up\", \"votes_funny\", \"author_playtime_forever\"]:\n",
        "    if c in df.columns:\n",
        "        df[c] = df[c].fillna(0)\n",
        "\n",
        "# 8) Simpan hasil cleaning (lokal)\n",
        "os.makedirs(os.path.dirname(LOCAL_OUT), exist_ok=True)\n",
        "df.to_csv(LOCAL_OUT, index=False)\n",
        "print(f\"\\nâœ… Data cleaned saved to: {LOCAL_OUT}\")\n",
        "print(\"Final shape:\", df.shape)\n"
      ],
      "metadata": {
        "id": "e6kbsSR9lQXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing **"
      ],
      "metadata": {
        "id": "RdUIcxOFvW-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# ðŸ”¹ DATA PREPROCESSING â€” tanpa Drive (lokal + fallback GitHub)\n",
        "#    posisi: setelah cleaning, sebelum modeling\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from urllib.parse import quote\n",
        "\n",
        "# --- 0) Path I/O: lokal terlebih dulu, kalau belum ada â†’ ambil dari GitHub ---\n",
        "IN_LOCAL  = \"data/processed/steam_reviews_clean.csv\"\n",
        "OUT_DIR   = \"data/processed\"\n",
        "OUT_PATH  = os.path.join(OUT_DIR, \"steam_reviews_preprocessed.csv\")\n",
        "\n",
        "# (fallback GitHub) â€” sesuaikan nama file jika beda\n",
        "BASE_URL  = \"https://raw.githubusercontent.com/fajaralgii04/bigdata-uts-ganjil-2526-/main/data/semua%20csv%20(gabungan)/\"\n",
        "GH_FILE   = \"steam_reviews_clean.csv\"  # ganti jika nama di repo berbeda\n",
        "\n",
        "def read_csv_github(filename: str, **kwargs) -> pd.DataFrame:\n",
        "    url = BASE_URL + quote(filename)\n",
        "    print(f\"ðŸŒ Loading from GitHub: {url}\")\n",
        "    return pd.read_csv(url, **kwargs)\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# 1) Load & cek ringkas\n",
        "if os.path.exists(IN_LOCAL):\n",
        "    print(f\"ðŸ“‚ Loaded local: {IN_LOCAL}\")\n",
        "    df = pd.read_csv(IN_LOCAL)\n",
        "else:\n",
        "    df = read_csv_github(GH_FILE)\n",
        "\n",
        "print(\"âœ… Loaded shape:\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "\n",
        "# 2) Drop kolom yang tidak dipakai untuk model numerik awal\n",
        "drop_cols = [\"appid\", \"name\", \"release_date\", \"review\"]  # 'review' teks mentah (NLP nanti)\n",
        "df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
        "\n",
        "# 3) Pastikan tipe & tangani NaN kolom numerik inti\n",
        "num_cols = [\"word_count\", \"votes_up\", \"votes_funny\", \"author_playtime_forever\", \"price\"]\n",
        "for c in num_cols:\n",
        "    if c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# Target kolom\n",
        "if \"voted_up\" not in df.columns:\n",
        "    raise ValueError(\"Kolom 'voted_up' tidak ditemukan di dataset cleaned.\")\n",
        "df[\"voted_up\"] = df[\"voted_up\"].astype(bool)\n",
        "\n",
        "# 4) Feature engineering sederhana\n",
        "def bucket_wc(x: float) -> str:\n",
        "    return \"short\" if x < 20 else (\"medium\" if x < 100 else \"long\")\n",
        "\n",
        "df[\"review_length\"] = df[\"word_count\"].apply(bucket_wc)\n",
        "\n",
        "# 5) Encoding kategorikal\n",
        "le = LabelEncoder()\n",
        "df[\"review_length_encoded\"] = le.fit_transform(df[\"review_length\"])\n",
        "\n",
        "# 6) Scaling fitur numerik (standarisasi)\n",
        "scale_cols = [c for c in num_cols if c in df.columns]\n",
        "scaler = StandardScaler()\n",
        "df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
        "\n",
        "# 7) Simpan hasil preprocessed (lokal)\n",
        "df.to_csv(OUT_PATH, index=False)\n",
        "print(f\"ðŸ’¾ Saved preprocessed dataset: {OUT_PATH} | shape: {df.shape}\")\n",
        "\n",
        "# 8) (Opsional) Split trainâ€“test untuk klasifikasi 'voted_up'\n",
        "FEATURE_COLS = [c for c in (num_cols + [\"review_length_encoded\"]) if c in df.columns]\n",
        "TARGET_COL   = \"voted_up\"\n",
        "\n",
        "X = df[FEATURE_COLS].copy()\n",
        "y = df[TARGET_COL].astype(bool).copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Split done. Train: {len(X_train)}, Test: {len(X_test)}\")\n",
        "print(\"Features used:\", FEATURE_COLS)\n",
        "\n",
        "# Simpan split ke lokal (biar modeling tinggal load)\n",
        "X_train.to_csv(os.path.join(OUT_DIR, \"X_train.csv\"), index=False)\n",
        "X_test.to_csv(os.path.join(OUT_DIR, \"X_test.csv\"), index=False)\n",
        "y_train.to_csv(os.path.join(OUT_DIR, \"y_train.csv\"), index=False)\n",
        "y_test.to_csv(os.path.join(OUT_DIR, \"y_test.csv\"), index=False)\n",
        "print(\"ðŸ’¾ Saved train/test splits to:\", OUT_DIR)\n"
      ],
      "metadata": {
        "id": "Fv0nHaXxvdIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Muhammad Fajar Algifari (1103223119)**"
      ],
      "metadata": {
        "id": "G8X1hoet1peY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pertanyaan 1** (SMART â€“ Apa & Dimana)\n",
        "\n",
        "Pada periode 1 Januari â€“ 31 Maret 2025, seberapa kuat hubungan antara durasi bermain dan kemungkinan review positif pada setiap game?\n",
        "Game mana yang menunjukkan korelasi Pearson paling kuat antara durasi bermain dan tingkat kepuasan pemain?"
      ],
      "metadata": {
        "id": "1-j8sUV_2IIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# ðŸ”¹ EDA: Korelasi Playtime vs Review Positif per Game (Top 10)\n",
        "#    (lokal â†’ fallback GitHub)\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from urllib.parse import quote\n",
        "\n",
        "# --- 0) Sumber data: lokal dahulu, bila tidak ada â†’ GitHub raw ---\n",
        "LOCAL_CLEAN = \"data/processed/steam_reviews_clean.csv\"\n",
        "\n",
        "BASE_URL = \"https://raw.githubusercontent.com/fajaralgii04/bigdata-uts-ganjil-2526-/main/data/semua%20csv%20(gabungan)/\"\n",
        "GH_FILE  = \"steam_reviews_clean.csv\"   # sesuaikan jika beda nama\n",
        "\n",
        "def read_csv_github(filename: str, **kwargs) -> pd.DataFrame:\n",
        "    url = BASE_URL + quote(filename)\n",
        "    print(f\"ðŸŒ Loading from GitHub: {url}\")\n",
        "    return pd.read_csv(url, **kwargs)\n",
        "\n",
        "# === 1ï¸âƒ£ Load data ===\n",
        "if os.path.exists(LOCAL_CLEAN):\n",
        "    print(f\"ðŸ“‚ Loading local: {LOCAL_CLEAN}\")\n",
        "    df = pd.read_csv(LOCAL_CLEAN)\n",
        "else:\n",
        "    df = read_csv_github(GH_FILE)\n",
        "\n",
        "# === 2ï¸âƒ£ Filter time-bound (mis. Q1 2025) ===\n",
        "# jika timestamp_created sudah datetime, ini tetap aman (errors='coerce')\n",
        "df[\"timestamp_created\"] = pd.to_datetime(df[\"timestamp_created\"], errors=\"coerce\")\n",
        "df = df[(df[\"timestamp_created\"] >= \"2025-01-01\") & (df[\"timestamp_created\"] < \"2025-04-01\")]\n",
        "\n",
        "# === 3ï¸âƒ£ Pastikan tidak ada NaN di kolom utama ===\n",
        "need_cols = [\"name\", \"author_playtime_forever\", \"voted_up\"]\n",
        "for c in need_cols:\n",
        "    if c not in df.columns:\n",
        "        raise KeyError(f\"Kolom '{c}' tidak ditemukan di dataset.\")\n",
        "df = df.dropna(subset=need_cols)\n",
        "\n",
        "# === 4ï¸âƒ£ Hitung korelasi Pearson per game ===\n",
        "def safe_corr(g: pd.DataFrame) -> float:\n",
        "    # ubah voted_up â†’ int agar Pearson valid\n",
        "    x = g[\"author_playtime_forever\"]\n",
        "    y = g[\"voted_up\"].astype(int)\n",
        "    # butuh variasi > 1 untuk korelasi bermakna\n",
        "    if x.nunique() > 1 and y.nunique() > 1:\n",
        "        return x.corr(y)\n",
        "    return np.nan\n",
        "\n",
        "corrs = (\n",
        "    df.groupby(\"name\")\n",
        "      .apply(safe_corr)\n",
        "      .dropna()\n",
        "      .sort_values(ascending=False)\n",
        "      .head(10)\n",
        ")\n",
        "\n",
        "# === 5ï¸âƒ£ Plot ===\n",
        "plt.figure(figsize=(9, 5))\n",
        "corrs.plot(kind=\"bar\", color=\"#2ecc71\")  # hijau lembut\n",
        "plt.title(\"Korelasi Playtime vs Review Positif per Game (Top 10)\\nPeriode: Janâ€“Mar 2025\", fontsize=13)\n",
        "plt.ylabel(\"Korelasi Pearson\", fontsize=11)\n",
        "plt.grid(axis=\"y\", alpha=0.3)\n",
        "plt.xticks(rotation=70, ha=\"right\")\n",
        "\n",
        "# label nilai di atas batang\n",
        "for i, v in enumerate(corrs.values):\n",
        "    plt.text(i, v + (0.01 if v >= 0 else -0.02), f\"{v:.3f}\",\n",
        "             ha=\"center\", va=\"bottom\" if v >= 0 else \"top\", fontsize=9, color=\"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2RWTA3f32lHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pertanyaan 2** (SMART â€“ Kapan & Bagaimana)\n",
        "\n",
        "Selama periode 1 Januari â€“ 31 Maret 2025, bagaimana perubahan proporsi review positif (voted_up) dari minggu ke minggu, dan apakah terdapat pola tren peningkatan atau penurunan pada tingkat kepuasan pemain di berbagai game populer di platform Steam?"
      ],
      "metadata": {
        "id": "lt-JCDGf5VsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# ðŸ”¹ EDA: Tren Mingguan Proporsi Review Positif (Top 10 Game)\n",
        "#    (lokal â†’ fallback GitHub)\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.parse import quote\n",
        "\n",
        "# --- 0) Sumber data: lokal dahulu, bila tidak ada â†’ GitHub raw ---\n",
        "LOCAL_CLEAN = \"data/processed/steam_reviews_clean.csv\"\n",
        "\n",
        "BASE_URL = \"https://raw.githubusercontent.com/fajaralgii04/bigdata-uts-ganjil-2526-/main/data/semua%20csv%20(gabungan)/\"\n",
        "GH_FILE  = \"steam_reviews_clean.csv\"   # sesuaikan jika nama di repo berbeda\n",
        "\n",
        "def read_csv_github(filename: str, **kwargs) -> pd.DataFrame:\n",
        "    url = BASE_URL + quote(filename)\n",
        "    print(f\"ðŸŒ Loading from GitHub: {url}\")\n",
        "    return pd.read_csv(url, **kwargs)\n",
        "\n",
        "# === 1ï¸âƒ£ Load dataset ===\n",
        "if os.path.exists(LOCAL_CLEAN):\n",
        "    print(f\"ðŸ“‚ Loading local: {LOCAL_CLEAN}\")\n",
        "    df = pd.read_csv(LOCAL_CLEAN)\n",
        "else:\n",
        "    df = read_csv_github(GH_FILE)\n",
        "\n",
        "# === 2ï¸âƒ£ Konversi waktu & filter periode Janâ€“Mar 2025 ===\n",
        "df[\"timestamp_created\"] = pd.to_datetime(df[\"timestamp_created\"], errors=\"coerce\")\n",
        "df = df[(df[\"timestamp_created\"] >= \"2025-01-01\") & (df[\"timestamp_created\"] < \"2025-04-01\")]\n",
        "\n",
        "# === 3ï¸âƒ£ Ambil 10 game dengan review terbanyak ===\n",
        "top_games = df[\"name\"].value_counts().head(10).index\n",
        "df_top = df[df[\"name\"].isin(top_games)].copy()\n",
        "\n",
        "# === 4ï¸âƒ£ Tambah kolom minggu ke-n (ISO week) ===\n",
        "df_top[\"week\"] = df_top[\"timestamp_created\"].dt.isocalendar().week.astype(int)\n",
        "\n",
        "# === 5ï¸âƒ£ Hitung proporsi review positif per minggu per game ===\n",
        "weekly_trend = (\n",
        "    df_top.groupby([\"name\", \"week\"], as_index=False)[\"voted_up\"]\n",
        "          .mean()\n",
        "          .rename(columns={\"voted_up\": \"proporsi_positif\"})\n",
        "          .sort_values([\"name\", \"week\"])\n",
        ")\n",
        "\n",
        "# === 6ï¸âƒ£ Plot garis per game ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "for game, g in weekly_trend.groupby(\"name\"):\n",
        "    plt.plot(g[\"week\"], g[\"proporsi_positif\"], marker=\"o\", label=game)\n",
        "\n",
        "plt.title(\"Tren Mingguan Proporsi Review Positif (Top 10 Game)\\nPeriode: Janâ€“Mar 2025\", fontsize=13)\n",
        "plt.xlabel(\"Minggu ke- (ISO Week)\")\n",
        "plt.ylabel(\"Proporsi Review Positif\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend(title=\"Nama Game\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9RdSkGOP54MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pertanyaan 3** (SMART â€“ Mengapa & Siapa)\n",
        "\n",
        "Sejauh mana tingkat keterlibatan komunitas (jumlah interaksi seperti votes_up, votes_funny, dan word_count) berkorelasi dengan kemungkinan review positif (voted_up) pada platform Steam selama periode 1 Januari â€“ 31 Maret 2025?\n",
        "Game mana yang menunjukkan hubungan paling kuat antara keterlibatan pemain dan sentimen positif?"
      ],
      "metadata": {
        "id": "oKY-jCDJ9PkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# ðŸ”¹ EDA SMART Q3 â€” Engagement â†” Positivity (Janâ€“Mar 2025)\n",
        "#    (lokal â†’ fallback GitHub)\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.parse import quote\n",
        "\n",
        "# --- 0) Sumber data: lokal dahulu, bila tidak ada â†’ GitHub raw ---\n",
        "LOCAL_CLEAN = \"data/processed/steam_reviews_clean.csv\"\n",
        "\n",
        "BASE_URL = \"https://raw.githubusercontent.com/fajaralgii04/bigdata-uts-ganjil-2526-/main/data/semua%20csv%20(gabungan)/\"\n",
        "GH_FILE  = \"steam_reviews_clean.csv\"   # sesuaikan jika beda nama\n",
        "\n",
        "def read_csv_github(filename: str, **kwargs) -> pd.DataFrame:\n",
        "    url = BASE_URL + quote(filename)\n",
        "    print(f\"ðŸŒ Loading from GitHub: {url}\")\n",
        "    return pd.read_csv(url, **kwargs)\n",
        "\n",
        "# ---- 1) Load & Time-bound ----\n",
        "if os.path.exists(LOCAL_CLEAN):\n",
        "    print(f\"ðŸ“‚ Loading local: {LOCAL_CLEAN}\")\n",
        "    df = pd.read_csv(LOCAL_CLEAN, low_memory=False)\n",
        "else:\n",
        "    df = read_csv_github(GH_FILE, low_memory=False)\n",
        "\n",
        "# pastikan datetime\n",
        "df[\"timestamp_created\"] = pd.to_datetime(df[\"timestamp_created\"], errors=\"coerce\")\n",
        "\n",
        "# Periode SMART: Janâ€“Mar 2025 (end exclusive)\n",
        "start, end = \"2025-01-01\", \"2025-04-01\"\n",
        "df = df[(df[\"timestamp_created\"] >= start) & (df[\"timestamp_created\"] < end)].copy()\n",
        "\n",
        "# Pastikan kolom tersedia\n",
        "required = {\"name\",\"voted_up\",\"votes_up\",\"votes_funny\",\"word_count\"}\n",
        "missing = required - set(df.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "# Bersihkan NaN penting\n",
        "df = df.dropna(subset=[\"name\",\"voted_up\",\"votes_up\",\"votes_funny\",\"word_count\"]).copy()\n",
        "df[\"voted_up_int\"] = df[\"voted_up\"].astype(int)\n",
        "\n",
        "# ---- 2) Engagement score & winsorize (anti-outlier) ----\n",
        "# pastikan numerik aman (jaga-jaga)\n",
        "for c in [\"votes_up\",\"votes_funny\",\"word_count\"]:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
        "\n",
        "df[\"engagement_score\"] = (\n",
        "    df[\"votes_up\"].clip(lower=0)\n",
        "  + df[\"votes_funny\"].clip(lower=0)\n",
        "  + df[\"word_count\"].clip(lower=0)\n",
        ")\n",
        "\n",
        "# winsorize p99 agar korelasi tidak didominasi outlier panjang/viral\n",
        "if df[\"engagement_score\"].notna().sum() > 0:\n",
        "    p99 = df[\"engagement_score\"].quantile(0.99)\n",
        "else:\n",
        "    p99 = 0\n",
        "df[\"engagement_score_w\"] = np.minimum(df[\"engagement_score\"], p99)\n",
        "\n",
        "# ---- 3) Korelasi Pearson per game ----\n",
        "def safe_corr(g: pd.DataFrame):\n",
        "    x = g[\"engagement_score_w\"]\n",
        "    y = g[\"voted_up_int\"]\n",
        "    # syarat variasi & sampel\n",
        "    if x.nunique() < 2 or y.nunique() < 2 or len(g) < 20:\n",
        "        return np.nan\n",
        "    return x.corr(y)\n",
        "\n",
        "corrs = (\n",
        "    df.groupby(\"name\", group_keys=False)\n",
        "      .apply(safe_corr)\n",
        "      .dropna()\n",
        "      .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "top10 = corrs.head(10)\n",
        "\n",
        "# ---- 4) Plot 1: Top-10 korelasi per game ----\n",
        "plt.figure(figsize=(10,6))\n",
        "top10.plot(kind=\"bar\")\n",
        "plt.title(\"Korelasi Engagement vs Review Positif per Game (Top 10)\\nPeriode: Janâ€“Mar 2025\")\n",
        "plt.ylabel(\"Korelasi Pearson\")\n",
        "plt.xticks(rotation=70, ha=\"right\")\n",
        "plt.grid(axis=\"y\", alpha=0.3)\n",
        "# label angka\n",
        "for i, v in enumerate(top10.values):\n",
        "    plt.text(i, v + (0.01 if v >= 0 else -0.02), f\"{v:.3f}\",\n",
        "             ha=\"center\", va=\"bottom\" if v >= 0 else \"top\", fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---- 5) Plot 2: Positive rate per kuartil engagement (global) ----\n",
        "# Jika semua engagement sama, qcut bisa gagal â†’ handle dengan duplicates='drop'\n",
        "try:\n",
        "    df[\"eng_q\"] = pd.qcut(\n",
        "        df[\"engagement_score_w\"],\n",
        "        4,\n",
        "        labels=[\"Q1 (rendah)\",\"Q2\",\"Q3\",\"Q4 (tinggi)\"],\n",
        "        duplicates=\"drop\"\n",
        "    )\n",
        "except ValueError:\n",
        "    # fallback: semua nilai sama â†’ buat satu bucket\n",
        "    df[\"eng_q\"] = \"Q-all\"\n",
        "\n",
        "quart = (\n",
        "    df.groupby(\"eng_q\", as_index=False)[\"voted_up_int\"]\n",
        "      .agg(pos_rate=\"mean\", n=\"size\")\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.bar(quart[\"eng_q\"].astype(str), quart[\"pos_rate\"])\n",
        "plt.title(\"Proporsi Review Positif per Kuartil Engagement\\nPeriode: Janâ€“Mar 2025\")\n",
        "plt.ylabel(\"Proporsi Positif\")\n",
        "for i, (rate, n) in enumerate(zip(quart[\"pos_rate\"], quart[\"n\"])):\n",
        "    plt.text(i, rate + 0.01, f\"{rate:.2f} (n={n})\", ha=\"center\", fontsize=9)\n",
        "plt.ylim(0, 1.05)\n",
        "plt.grid(axis=\"y\", alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---- 6) Cetak ringkas angka kunci (untuk narasi) ----\n",
        "print(\"== Top-10 Game dengan Korelasi Engagementâ†”Positivity ==\")\n",
        "print(top10.to_frame(\"corr_engagement_positive\").round(3))\n",
        "\n",
        "print(\"\\n== Positive Rate per Kuartil Engagement ==\")\n",
        "print(\n",
        "    quart.assign(**{\"pos_rate(%)\": (quart[\"pos_rate\"]*100).round(2)})\n",
        "         .drop(columns=\"pos_rate\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "hbqWVZpv9g5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Auldy Ranayu Sanny Prahasty Rachman (1103223216)**"
      ],
      "metadata": {
        "id": "9MFr6DagA_Px"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pertanyaan 1 (SMART â€“ Apa & Mengapa)\n",
        "\n",
        "Pada periode 1 Januari â€“ 31 Maret 2025, bagaimana hubungan antara jumlah kata dalam review (word_count) dengan kemungkinan review positif (voted_up), dan mengapa hubungan tersebut dapat terjadi?"
      ],
      "metadata": {
        "id": "l6XS5aepBS5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# ðŸ”¹ EDA: Proporsi Review Positif vs Panjang Review (Janâ€“Mar 2025)\n",
        "#    (lokal â†’ fallback GitHub)\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.parse import quote\n",
        "\n",
        "# --- 0) Sumber data: lokal dahulu, bila tidak ada â†’ GitHub raw ---\n",
        "LOCAL_CLEAN = \"data/processed/steam_reviews_clean.csv\"\n",
        "\n",
        "BASE_URL = \"https://raw.githubusercontent.com/fajaralgii04/bigdata-uts-ganjil-2526-/main/data/semua%20csv%20(gabungan)/\"\n",
        "GH_FILE  = \"steam_reviews_clean.csv\"   # sesuaikan jika beda nama\n",
        "\n",
        "def read_csv_github(filename: str, **kwargs) -> pd.DataFrame:\n",
        "    url = BASE_URL + quote(filename)\n",
        "    print(f\"ðŸŒ Loading from GitHub: {url}\")\n",
        "    return pd.read_csv(url, **kwargs)\n",
        "\n",
        "# 1ï¸âƒ£ Load data & filter waktu\n",
        "if os.path.exists(LOCAL_CLEAN):\n",
        "    print(f\"ðŸ“‚ Loading local: {LOCAL_CLEAN}\")\n",
        "    df = pd.read_csv(LOCAL_CLEAN)\n",
        "else:\n",
        "    df = read_csv_github(GH_FILE)\n",
        "\n",
        "need_cols = {\"timestamp_created\",\"word_count\",\"voted_up\"}\n",
        "missing = need_cols - set(df.columns)\n",
        "if missing:\n",
        "    raise KeyError(f\"Kolom wajib hilang: {missing}\")\n",
        "\n",
        "df[\"timestamp_created\"] = pd.to_datetime(df[\"timestamp_created\"], errors=\"coerce\")\n",
        "df = df[(df[\"timestamp_created\"] >= \"2025-01-01\") & (df[\"timestamp_created\"] < \"2025-04-01\")].copy()\n",
        "\n",
        "# 2ï¸âƒ£ Kategorisasi panjang review (guard NaN & tipe)\n",
        "df[\"word_count\"] = pd.to_numeric(df[\"word_count\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "def cat_len(x: float) -> str:\n",
        "    if x < 20: return \"Short\"\n",
        "    elif x < 100: return \"Medium\"\n",
        "    else: return \"Long\"\n",
        "\n",
        "df[\"review_length_cat\"] = df[\"word_count\"].apply(cat_len)\n",
        "\n",
        "# 3ï¸âƒ£ Hitung proporsi review positif per kategori (pastikan boolean)\n",
        "df[\"voted_up\"] = df[\"voted_up\"].astype(bool)\n",
        "order = [\"Short\", \"Medium\", \"Long\"]\n",
        "ratio = (\n",
        "    df.groupby(\"review_length_cat\", as_index=True)[\"voted_up\"]\n",
        "      .mean()\n",
        "      .reindex(order)\n",
        ")\n",
        "\n",
        "# 4ï¸âƒ£ Visualisasi\n",
        "plt.figure(figsize=(7,4))\n",
        "ratio.plot(kind=\"bar\", color=[\"#e74c3c\", \"#f39c12\", \"#27ae60\"])\n",
        "plt.title(\"Proporsi Review Positif Berdasarkan Panjang Review (Janâ€“Mar 2025)\")\n",
        "plt.ylabel(\"Proporsi Review Positif\")\n",
        "plt.xlabel(\"Kategori Panjang Review\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "# Label nilai\n",
        "for i, v in enumerate(ratio.values):\n",
        "    if pd.notna(v):\n",
        "        plt.text(i, min(v + 0.02, 1.02), f\"{v:.2f}\", ha=\"center\", fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RM1GXwqSBV5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pertanyaan 2 (SMART â€“ Dimana & Bagaimana)\n",
        "\n",
        "Pada periode 1 Januari â€“ 31 Maret 2025, game mana yang memiliki tingkat keterlibatan (engagement) tertinggi berdasarkan jumlah interaksi pemain (votes_up dan votes_funny), dan bagaimana keterlibatan tersebut tercermin dalam jumlah rata-rata vote yang diterima?\n"
      ],
      "metadata": {
        "id": "mnRbDxFHBgnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# ðŸ”¹ EDA: Top 10 Game dengan Engagement Tertinggi (Votes Up & Funny)\n",
        "#    (lokal â†’ fallback GitHub)\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.parse import quote\n",
        "\n",
        "# --- 0) Sumber data: lokal dahulu, bila tidak ada â†’ GitHub raw ---\n",
        "LOCAL_CLEAN = \"data/processed/steam_reviews_clean.csv\"\n",
        "BASE_URL    = \"https://raw.githubusercontent.com/fajaralgii04/bigdata-uts-ganjil-2526-/main/data/semua%20csv%20(gabungan)/\"\n",
        "GH_FILE     = \"steam_reviews_clean.csv\"   # sesuaikan jika beda nama\n",
        "\n",
        "def read_csv_github(filename: str, **kwargs) -> pd.DataFrame:\n",
        "    url = BASE_URL + quote(filename)\n",
        "    print(f\"ðŸŒ Loading from GitHub: {url}\")\n",
        "    return pd.read_csv(url, **kwargs)\n",
        "\n",
        "# 1ï¸âƒ£ Load data & filter periode waktu\n",
        "if os.path.exists(LOCAL_CLEAN):\n",
        "    print(f\"ðŸ“‚ Loading local: {LOCAL_CLEAN}\")\n",
        "    df = pd.read_csv(LOCAL_CLEAN)\n",
        "else:\n",
        "    df = read_csv_github(GH_FILE)\n",
        "\n",
        "need_cols = {\"timestamp_created\",\"name\",\"votes_up\",\"votes_funny\"}\n",
        "missing = need_cols - set(df.columns)\n",
        "if missing:\n",
        "    raise KeyError(f\"Kolom wajib hilang: {missing}\")\n",
        "\n",
        "df[\"timestamp_created\"] = pd.to_datetime(df[\"timestamp_created\"], errors=\"coerce\")\n",
        "df = df[(df[\"timestamp_created\"] >= \"2025-01-01\") & (df[\"timestamp_created\"] < \"2025-04-01\")].copy()\n",
        "\n",
        "# jaga numerik\n",
        "for c in [\"votes_up\",\"votes_funny\"]:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# 2ï¸âƒ£ Hitung engagement rata-rata per game\n",
        "eng = (\n",
        "    df.groupby(\"name\")[[\"votes_up\", \"votes_funny\"]]\n",
        "      .mean()\n",
        "      .sort_values(\"votes_up\", ascending=False)\n",
        "      .head(10)\n",
        ")\n",
        "\n",
        "# 3ï¸âƒ£ Visualisasi top 10 game dengan engagement tertinggi\n",
        "ax = eng.plot(kind=\"bar\", figsize=(10,5))\n",
        "plt.title(\"Top 10 Game dengan Engagement Tertinggi (Votes Up & Funny)\\nPeriode: Janâ€“Mar 2025\", fontsize=12)\n",
        "plt.ylabel(\"Rata-rata Jumlah Vote per Review\")\n",
        "plt.xlabel(\"Nama Game\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "# Tambahkan label angka di atas tiap batang (akurasi posisi antar kolom)\n",
        "n_groups = len(eng)\n",
        "n_cols   = len(eng.columns)  # 2 kolom: votes_up & votes_funny\n",
        "x = np.arange(n_groups)\n",
        "barw = 0.8 / n_cols\n",
        "\n",
        "for i, col in enumerate(eng.columns):\n",
        "    yvals = eng[col].values\n",
        "    for xi, yi in zip(x, yvals):\n",
        "        xpos = xi - 0.4 + (i + 0.5) * barw   # offset per kolom dalam grup\n",
        "        plt.text(xpos, yi + max(0.02, 0.02*yi), f\"{yi:.1f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GlfA6pQIBhUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pertanyaan 3 (SMART â€“ Kapan & Siapa):\n",
        "\n",
        "Pada periode Januariâ€“Desember 2025, kapan aktivitas pemain paling tinggi terjadi, dan game mana yang memiliki pertumbuhan jumlah review paling konsisten sepanjang tahun?\n"
      ],
      "metadata": {
        "id": "cFRNDl-_B3Rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# ðŸ”¹ EDA: Tren Aktivitas Review Bulanan (Top 5 Game) â€” Janâ€“Mar 2025\n",
        "#    (lokal â†’ fallback GitHub)\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.parse import quote\n",
        "\n",
        "# --- 0) Sumber data: lokal dahulu, bila tidak ada â†’ GitHub raw ---\n",
        "LOCAL_CLEAN = \"data/processed/steam_reviews_clean.csv\"\n",
        "BASE_URL    = \"https://raw.githubusercontent.com/fajaralgii04/bigdata-uts-ganjil-2526-/main/data/semua%20csv%20(gabungan)/\"\n",
        "GH_FILE     = \"steam_reviews_clean.csv\"   # sesuaikan jika beda nama\n",
        "\n",
        "def read_csv_github(filename: str, **kwargs) -> pd.DataFrame:\n",
        "    url = BASE_URL + quote(filename)\n",
        "    print(f\"ðŸŒ Loading from GitHub: {url}\")\n",
        "    return pd.read_csv(url, **kwargs)\n",
        "\n",
        "# 1ï¸âƒ£ Load data & konversi waktu\n",
        "if os.path.exists(LOCAL_CLEAN):\n",
        "    print(f\"ðŸ“‚ Loading local: {LOCAL_CLEAN}\")\n",
        "    df = pd.read_csv(LOCAL_CLEAN)\n",
        "else:\n",
        "    df = read_csv_github(GH_FILE)\n",
        "\n",
        "need_cols = {\"timestamp_created\",\"name\"}\n",
        "missing = need_cols - set(df.columns)\n",
        "if missing:\n",
        "    raise KeyError(f\"Kolom wajib hilang: {missing}\")\n",
        "\n",
        "df[\"timestamp_created\"] = pd.to_datetime(df[\"timestamp_created\"], errors=\"coerce\")\n",
        "\n",
        "# 2ï¸âƒ£ Tambah kolom bulan (Month-Year)\n",
        "df[\"month\"] = df[\"timestamp_created\"].dt.to_period(\"M\").dt.to_timestamp()\n",
        "\n",
        "# 3ï¸âƒ£ Filter periode Januariâ€“Maret 2025\n",
        "df = df[(df[\"month\"] >= \"2025-01-01\") & (df[\"month\"] < \"2025-04-01\")].copy()\n",
        "\n",
        "# 4ï¸âƒ£ Agregasi jumlah review per game per bulan\n",
        "monthly_game = df.groupby([\"name\", \"month\"], as_index=False).size()\n",
        "pivot = monthly_game.pivot(index=\"month\", columns=\"name\", values=\"size\").fillna(0)\n",
        "\n",
        "# 5ï¸âƒ£ Pilih 5 game dengan total review terbanyak\n",
        "top_games = pivot.sum(axis=0).sort_values(ascending=False).head(5).index\n",
        "\n",
        "# 6ï¸âƒ£ Visualisasi tren bulanan review\n",
        "ax = pivot[top_games].plot(figsize=(10,5), linewidth=2)\n",
        "plt.title(\"Tren Aktivitas Review Bulanan (Top 5 Game)\\nPeriode: Janâ€“Mar 2025\")\n",
        "plt.xlabel(\"Bulan\")\n",
        "plt.ylabel(\"Jumlah Review\")\n",
        "plt.legend(title=\"Game\", bbox_to_anchor=(1.02,1), loc=\"upper left\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "631C4MoaB5T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "UvM19l1OGr4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "l-deZlw1GsTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "WIUFIQDPGs5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "R0RcbWj1GtaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KLASIFIKASI\n",
        "\n",
        "Memprediksi apakah review pemain bersifat positif (voted_up=True) atau negatif (False)."
      ],
      "metadata": {
        "id": "tLQ3POUVtMW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ðŸ”¹ KLASIFIKASI: SVM dan RANDOM FOREST\n",
        "#    (lokal â†’ fallback GitHub)\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from urllib.parse import quote\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 0) Helper loader: lokal dulu, kalau tidak ada â†’ GitHub raw ---\n",
        "BASE_URL = \"https://raw.githubusercontent.com/fajaralgii04/bigdata-uts-ganjil-2526-/main/data/semua%20csv%20(gabungan)/\"\n",
        "\n",
        "def read_csv_github(filename: str, **kwargs) -> pd.DataFrame:\n",
        "    url = BASE_URL + quote(filename)\n",
        "    print(f\"ðŸŒ Loading from GitHub: {url}\")\n",
        "    return pd.read_csv(url, **kwargs)\n",
        "\n",
        "def load_split(name: str) -> pd.DataFrame:\n",
        "    local_path = f\"data/processed/{name}.csv\"\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"ðŸ“‚ Loading local: {local_path}\")\n",
        "        return pd.read_csv(local_path)\n",
        "    # fallback ke GitHub\n",
        "    return read_csv_github(f\"{name}.csv\")\n",
        "\n",
        "# 1ï¸âƒ£ Load data hasil preprocessing (X/y split)\n",
        "X_train = load_split(\"X_train\")\n",
        "X_test  = load_split(\"X_test\")\n",
        "y_train = load_split(\"y_train\").squeeze()\n",
        "y_test  = load_split(\"y_test\").squeeze()\n",
        "\n",
        "# jaga tipe target boolean\n",
        "if y_train.dtype != bool:\n",
        "    y_train = y_train.astype(bool)\n",
        "if y_test.dtype != bool:\n",
        "    y_test = y_test.astype(bool)\n",
        "\n",
        "# 2ï¸âƒ£ Model SVM\n",
        "svm = SVC(kernel=\"rbf\", random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "# 3ï¸âƒ£ Model Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# 4ï¸âƒ£ Evaluasi fungsi umum\n",
        "def eval_model(name, y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    pre = precision_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    f1  = f1_score(y_true, y_pred)\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(f\"Akurasi : {acc:.3f}\")\n",
        "    print(f\"Presisi : {pre:.3f}\")\n",
        "    print(f\"Recall  : {rec:.3f}\")\n",
        "    print(f\"F1-Score: {f1:.3f}\")\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(4.5,3.8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", cbar=False)\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.xlabel(\"Prediksi\")\n",
        "    plt.ylabel(\"Aktual\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 5ï¸âƒ£ Evaluasi kedua model\n",
        "eval_model(\"SVM\", y_test, y_pred_svm)\n",
        "eval_model(\"Random Forest\", y_test, y_pred_rf)\n"
      ],
      "metadata": {
        "id": "1GuvArXNGxrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. REGRESI â€” LINEAR REGRESSION & XGBOOST\n",
        "Tujuan:\n",
        "\n",
        "Memprediksi jumlah upvotes (votes_up) yang diterima oleh sebuah review."
      ],
      "metadata": {
        "id": "h5MoyI58uuoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ðŸ”¹ REGRESI: LINEAR REGRESSION & XGBOOST\n",
        "#    (lokal â†’ fallback GitHub) + anti data leakage\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.parse import quote\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# --- 0) Loader: lokal dulu, kalau tidak ada â†’ GitHub raw ---\n",
        "BASE_URL = \"https://raw.githubusercontent.com/fajaralgii04/bigdata-uts-ganjil-2526-/main/data/semua%20csv%20(gabungan)/\"\n",
        "\n",
        "def read_csv_github(filename: str, **kwargs) -> pd.DataFrame:\n",
        "    url = BASE_URL + quote(filename)\n",
        "    print(f\"ðŸŒ Loading from GitHub: {url}\")\n",
        "    return pd.read_csv(url, **kwargs)\n",
        "\n",
        "def load_split(name: str) -> pd.DataFrame:\n",
        "    local_path = f\"data/processed/{name}.csv\"\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"ðŸ“‚ Loading local: {local_path}\")\n",
        "        return pd.read_csv(local_path)\n",
        "    return read_csv_github(f\"{name}.csv\")\n",
        "\n",
        "# 1ï¸âƒ£ Load data split (hasil preprocessing)\n",
        "X_train = load_split(\"X_train\")\n",
        "X_test  = load_split(\"X_test\")\n",
        "\n",
        "# Target: votes_up  â†’ HINDARI LEAKAGE: drop 'votes_up' dari fitur\n",
        "if \"votes_up\" not in X_train.columns or \"votes_up\" not in X_test.columns:\n",
        "    raise KeyError(\"Kolom 'votes_up' tidak ditemukan di X_train/X_test. Pastikan preprocessing menyertakan kolom ini.\")\n",
        "\n",
        "y_train_reg = X_train[\"votes_up\"].astype(float).copy()\n",
        "y_test_reg  = X_test[\"votes_up\"].astype(float).copy()\n",
        "\n",
        "X_train_reg = X_train.drop(columns=[\"votes_up\"], errors=\"ignore\").copy()\n",
        "X_test_reg  = X_test.drop(columns=[\"votes_up\"], errors=\"ignore\").copy()\n",
        "\n",
        "# 2ï¸âƒ£ Model Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_reg, y_train_reg)\n",
        "pred_lr = lr.predict(X_test_reg)\n",
        "\n",
        "# 3ï¸âƒ£ Model XGBoost\n",
        "xgb = XGBRegressor(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    random_state=42,\n",
        ")\n",
        "xgb.fit(X_train_reg, y_train_reg)\n",
        "pred_xgb = xgb.predict(X_test_reg)\n",
        "\n",
        "# 4ï¸âƒ£ Evaluasi fungsi umum\n",
        "def eval_reg(name, y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(f\"MSE : {mse:.3f}\")\n",
        "    print(f\"RMSE: {rmse:.3f}\")\n",
        "    print(f\"RÂ²  : {r2:.3f}\")\n",
        "\n",
        "# 5ï¸âƒ£ Evaluasi hasil model\n",
        "eval_reg(\"Linear Regression\", y_test_reg, pred_lr)\n",
        "eval_reg(\"XGBoost\", y_test_reg, pred_xgb)\n",
        "\n",
        "# ==========================================\n",
        "# ðŸ”¹ VISUALISASI HASIL REGRESI\n",
        "# ==========================================\n",
        "\n",
        "# Scatter plot: aktual vs prediksi (kedua model)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(y_test_reg, pred_lr, alpha=0.5, label=\"Linear Regression\")\n",
        "plt.scatter(y_test_reg, pred_xgb, alpha=0.5, label=\"XGBoost\")\n",
        "plt.xlabel(\"Nilai Aktual (votes_up)\")\n",
        "plt.ylabel(\"Nilai Prediksi (votes_up)\")\n",
        "plt.title(\"Perbandingan Prediksi vs Nilai Aktual\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Garis diagonal (prediksi ideal) vs XGBoost\n",
        "plt.figure(figsize=(6, 6))\n",
        "lo, hi = float(y_test_reg.min()), float(y_test_reg.max())\n",
        "plt.plot([lo, hi], [lo, hi], linestyle=\"--\", label=\"Prediksi Ideal\")\n",
        "plt.scatter(y_test_reg, pred_xgb, alpha=0.5, label=\"XGBoost Prediction\")\n",
        "plt.xlabel(\"Nilai Aktual (votes_up)\")\n",
        "plt.ylabel(\"Nilai Prediksi (votes_up)\")\n",
        "plt.title(\"Kesesuaian Prediksi XGBoost terhadap Nilai Aktual\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a8SoAfIWuwy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLUSTERING â€” K-MEANS & DBSCAN\n",
        "Tujuan:\n",
        "\n",
        "Mengelompokkan game berdasarkan pola engagement pemain (unsupervised learning)."
      ],
      "metadata": {
        "id": "8eTM3w37u7AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ðŸ”¹ CLUSTERING: K-MEANS & DBSCAN\n",
        "#    (lokal â†’ fallback GitHub) + guard silhouette\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.parse import quote\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --- 0) Loader: lokal dulu, kalau tidak ada â†’ GitHub raw ---\n",
        "LOCAL_PREP = \"data/processed/steam_reviews_preprocessed.csv\"\n",
        "BASE_URL   = \"https://raw.githubusercontent.com/fajaralgii04/bigdata-uts-ganjil-2526-/main/data/semua%20csv%20(gabungan)/\"\n",
        "GH_FILE    = \"steam_reviews_preprocessed.csv\"  # sesuaikan jika beda nama\n",
        "\n",
        "def read_csv_github(filename: str, **kwargs) -> pd.DataFrame:\n",
        "    url = BASE_URL + quote(filename)\n",
        "    print(f\"ðŸŒ Loading from GitHub: {url}\")\n",
        "    return pd.read_csv(url, **kwargs)\n",
        "\n",
        "# 1ï¸âƒ£ Load dataset numerik\n",
        "if os.path.exists(LOCAL_PREP):\n",
        "    print(f\"ðŸ“‚ Loading local: {LOCAL_PREP}\")\n",
        "    X = pd.read_csv(LOCAL_PREP)\n",
        "else:\n",
        "    X = read_csv_github(GH_FILE)\n",
        "\n",
        "need_cols = [\"word_count\",\"votes_up\",\"votes_funny\",\"author_playtime_forever\",\"price\"]\n",
        "missing = set(need_cols) - set(X.columns)\n",
        "if missing:\n",
        "    raise KeyError(f\"Kolom wajib hilang: {missing}\")\n",
        "\n",
        "X_num = X[need_cols].copy()\n",
        "# jaga numerik + scale (jika file belum di-scale)\n",
        "for c in need_cols:\n",
        "    X_num[c] = pd.to_numeric(X_num[c], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# jika kamu yakin file sudah distandardize di preprocessing, blok scaler bisa dilewati.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_num)\n",
        "\n",
        "# 2ï¸âƒ£ Elbow Method (K=2..9)\n",
        "inertia, K_range = [], range(2, 10)\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(list(K_range), inertia, marker=\"o\")\n",
        "plt.title(\"Elbow Method untuk Menentukan Jumlah Cluster\")\n",
        "plt.xlabel(\"Jumlah Cluster (k)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3ï¸âƒ£ Jalankan model K-Means dan DBSCAN\n",
        "kmeans = KMeans(n_clusters=3, n_init=\"auto\", random_state=42)\n",
        "labels_km = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels_db = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# 4ï¸âƒ£ Evaluasi Silhouette Score (hanya valid jika â‰¥2 cluster berbeda)\n",
        "def safe_silhouette(Xarr, labels, name):\n",
        "    uniq = np.unique(labels)\n",
        "    # DBSCAN bisa hasilkan semua -1 (noise) atau cuma 1 cluster â†’ tidak valid\n",
        "    if len(uniq) < 2 or (len(uniq) == 2 and -1 in uniq and (labels != -1).sum() == 0):\n",
        "        print(f\"{name}: Silhouette tidak dapat dihitung (cluster < 2 / semua noise).\")\n",
        "        return np.nan\n",
        "    try:\n",
        "        score = silhouette_score(Xarr, labels)\n",
        "        print(f\"{name}: Silhouette Score = {score:.3f}\")\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        print(f\"{name}: Gagal hitung silhouette â†’ {e}\")\n",
        "        return np.nan\n",
        "\n",
        "score_km = safe_silhouette(X_scaled, labels_km, \"K-Means\")\n",
        "score_db = safe_silhouette(X_scaled, labels_db, \"DBSCAN\")\n"
      ],
      "metadata": {
        "id": "XVw7iClau9sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vNtajH7AI24"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}